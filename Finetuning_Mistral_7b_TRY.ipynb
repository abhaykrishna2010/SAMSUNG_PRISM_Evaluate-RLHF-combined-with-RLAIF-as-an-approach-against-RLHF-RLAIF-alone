{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHlAbqzDFDq"
      },
      "source": [
        "# Fine-tune Mistral for Social Media Analysis\n",
        "\n",
        "\n",
        "This code notebook makes it possible to fine-tune Mistral on a set of social media annotation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBw0yow-oF_T"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7SQ9DQiZRMJ"
      },
      "source": [
        "We connect to the drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install transformers and peft separately from the latest version on Github. Otherwhise, you will miss key metadata for Mistral support."
      ],
      "metadata": {
        "id": "ZHnjOCNtDmRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "hcZMuljzBvme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the correct version of tensorflow:"
      ],
      "metadata": {
        "id": "x2b86798nJi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.14"
      ],
      "metadata": {
        "id": "Cy4AYfzoSJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VMWXkg7ohLp"
      },
      "source": [
        "We install the other extensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate bitsandbytes trl guardrail-ml tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the libraries"
      ],
      "metadata": {
        "id": "cfO7_0k5D21-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1795700-522e-46a9-a575-30d5ae52cbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    LlamaTokenizerFast\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DU3VdvYpRbi"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqw8JhmcfOOn"
      },
      "source": [
        "The important hyperparameters first:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_device_train_batch_size = 4 #Number of texts sent in one batch: higher will mean quicker epochs, lower less vram.\n",
        "learning_rate = 2e-4 #Rate of memorization and also amnesia of past knowledge. High value are preferable for annotations.\n",
        "max_seq_length = 1024 #Context window: not necessarily big for analytical LLMs of social media expression.\n",
        "\n",
        "# The name of Mistral model\n",
        "model_name = \"mistral-7b-v0.1\"\n",
        "\n",
        "# Le name of the new model.\n",
        "new_model_name = \"mistral-7b-sna\"\n",
        "\n",
        "# The number of steps.\n",
        "# I prefer this to the number of epochs (easier to manage and anticipate the time it takes to finetune)\n",
        "max_steps = 500\n",
        "\n",
        "# Saving steps. Useful when there is an issue with fine-tuning: your can easily restart.\n",
        "save_steps = 100\n",
        "\n",
        "# The output directory where the model predictions and checkpoints will be written\n",
        "output_dir = \"./mistral-7b-sna\"\n",
        "\n",
        "# Tensorboard logs\n",
        "tb_log_dir = \"./mistral-7b-sna/logs\""
      ],
      "metadata": {
        "id": "sMe7P6nhnT1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other hyperparameters (no need to change them normally):"
      ],
      "metadata": {
        "id": "-oqSSlRTn9Rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "# Base parameters\n",
        "local_rank = -1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "max_grad_norm = 0.3\n",
        "weight_decay = 0.001\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "group_by_length = True\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Activate nested quantization for 4-bit base models\n",
        "use_nested_quant = False\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4=\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16 training\n",
        "fp16 = True\n",
        "\n",
        "# Enable bf16 training\n",
        "bf16 = False\n",
        "\n",
        "# Use packing dataset creating\n",
        "packing = False\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Optimizer to use, original is paged_adamw_32bit\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Fraction of steps to do a warmup for\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length (saves memory and speeds up training considerably)\n",
        "group_by_length = True\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 1\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Visualize training\n",
        "report_to = \"tensorboard\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ubBtAc7i0X9"
      },
      "source": [
        "We load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a light version (in 4-bit) to speed up training."
      ],
      "metadata": {
        "id": "YOO_luwhgdS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "1bacea8f68dd4d2cab09775540f7786a",
            "1bb052c616c84851a0991c2730753d26",
            "fbc861bb471f481a8b7cbb9d90a2b9bd",
            "22ee3080750f4070bea559a22107046a",
            "69f4669e71b64cb08633b33fe6815473",
            "e5638ff078eb43c2a126e8de3bf97b4e",
            "f4f6e6d0f5e84ac3b8a5f0c0175c012f",
            "6ef99f3030f941f48e90a7cb07511374",
            "efd96969d87148fa8109b573ec16217f",
            "1b8c483e05a643b09b030b6c3375a107",
            "fae0a3e73eeb410195d04e6e7f85d44f"
          ]
        },
        "id": "OJXpOgBFuSrc",
        "outputId": "6d5901e1-8b43-42c5-f780-148c6b9838e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16, you can accelerate training with the argument --bf16\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bacea8f68dd4d2cab09775540f7786a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the tokenizer and the peft configuration. Notice you have to specify the target modules as peft is not yet fully updated for Mistral.\n",
        "\n",
        "Also using the llama fast tokenizer but not sure if this is the best idea…"
      ],
      "metadata": {
        "id": "omBH_dCfgnLl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRjsfVsQk0MD"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    inference_mode=False,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules = [\"q_proj\", \"v_proj\"] #There are options to deepen the finetuning by unfreezing more weights but with a cost in performance\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True)\n",
        "#tokenizer = LlamaTokenizerFast.from_pretrained(model_name, add_eos_token=True, from_slow=True)\n",
        "\n",
        "# This is the fix for fp16 training\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8_1EpkupVDo"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should put the dataset in the same directory as the models (so Mistral here)"
      ],
      "metadata": {
        "id": "0irDlQvPhFH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data in a custom format. Here 'full_text' is the input and 'analysis' the expected output for analysis. May have to be changed for custom fields."
      ],
      "metadata": {
        "id": "CBVQVF7ZguE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWbzDeSKmakC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324f2cec-e7a3-4a96-9f47-824f81a724b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 4272\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def format_custom(sample):\n",
        "    instruction = f\"<s>Text: {sample['full_text']} \\n\\n### Analysis:\\n\\n\"\n",
        "    context = None\n",
        "    response = f\"{sample['analysis']}\"\n",
        "    # join all the parts together\n",
        "    prompt = \"\".join([i for i in [instruction, context, response] if i is not None])\n",
        "    return prompt\n",
        "\n",
        "# template dataset to add prompt to each sample\n",
        "def template_dataset(sample):\n",
        "    sample[\"text\"] = f\"{format_custom(sample)}{tokenizer.eos_token}\"\n",
        "    return sample\n",
        "\n",
        "# Loadng the dataset.\n",
        "data_files = {\"train\": \"brahe_instructions.json\"}\n",
        "dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "\n",
        "#Transformation du dataset pour utiliser le format guanaco\n",
        "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sample of the dataset:"
      ],
      "metadata": {
        "id": "ThdPKP_Qocrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsylN-itwa4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c06128d-75ab-45b6-c3eb-3b8c13494477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>Text: We found that the wave had actually borne the boat on its crest from the beach into the woods, and there launched it into the heart of this bush; which was extremely fortunate, for had it been tossed against a rock or a tree, it would have been dashed to pieces, whereas it had not received the smallest injury. It was no easy matter, however, to get it out of the bush and down to the sea again. This cost us two days of hard labour to accomplish. We had also much ado to clear away the rubbish from before the bower, and spent nearly a week in constant labour ere we got the neighbourhood to look as clean and orderly as before; for the uprooted bushes and seaweed that lay on the beach formed a more dreadfully confused-looking mass than one who had not seen the place after the inundation could conceive. Before leaving the subject, I may mention, for the sake of those who interest themselves in the curious natural phenomena of our world, that this gigantic wave occurs regularly on some of the islands of the Pacific once, and sometimes twice, in the year. I heard this stated by the missionaries during my career in those seas. They could not tell me whether it visited all of the islands, but I was certainly assured that it occurred periodically in some of them. After we had got our home put to rights, and cleared of the debris of the inundation, we again turned our thoughts to paying the penguins a visit. The boat was therefore overhauled and a few repairs done. \\n\\n### Analysis:\\n\\nSummary: The narrator and their companions find their boat washed ashore by a giant wave, but luckily it is undamaged. They spend several days clearing debris and then decide to visit the penguins.\\nNarrative arc: No specific arch mentioned\\nEnunciation: First-person narrative\\nTone: Informative\\nGenre: Adventure\\nIntertextuality: Scientific article\\nSpeech standard: Informal\\nLiterary form: Description of events\\nActive character: The narrator and their companions\\nDiegetic time: Two days of hard labor, nearly a week of constant labor\\nFuzzy place: The beach, the woods</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset[40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjy2W_6pXVJ"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We launch the training. Should take 1-2 hours with the default settings."
      ],
      "metadata": {
        "id": "eX0I4oqchkRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "hGHdUKwBCx8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usy6vtIXf02m"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "#trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the model"
      ],
      "metadata": {
        "id": "ess77MxshpBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZZdXiPbnGNq"
      },
      "outputs": [],
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRXX09_uLGN"
      },
      "source": [
        "We merge the model and the LORA to get inference speed up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "db6175917fdf481693d30d258cd1c7ad",
            "d543faeb3a0647d8833c4e4417c6f88b",
            "9353efae551e413c8f95259906b9deeb",
            "fed382f6cb0a4b839ace317b8b96e5b3",
            "3639468703a043b5bb819076f45f375d",
            "392b15d35cfc45398e64cf6e315a4bc6",
            "1a633383d08a4eb28e7bad389c2fda55",
            "50119634a30e4a3db374662df8f61e03",
            "db5aa8df2ab14f49b7bc08191ff11227",
            "f7cbef2170ef466bb646906f97b6a0ee",
            "e61c36bd89e74d819b6a81f8aeeef544"
          ]
        },
        "id": "Pkm5WHGguOlo",
        "outputId": "a700dab0-5fbe-407a-b95d-1a46aeca8eee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db6175917fdf481693d30d258cd1c7ad"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(new_model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_merged_dir = os.path.join(new_model_name, \"final_merged_checkpoint\")\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)"
      ],
      "metadata": {
        "id": "aFSkF69w7MYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We export the tokenizer files."
      ],
      "metadata": {
        "id": "uyyfEwHY2ik4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"mistral-7b-v0.1/tokenizer.json\" \"mistral-7b-sna/final_merged_checkpoint/tokenizer.json\"\n",
        "!cp \"mistral-7b-v0.1/tokenizer.model\" \"mistral-7b-sna/final_merged_checkpoint/tokenizer.model\"\n",
        "!cp \"mistral-7b-v0.1/tokenizer_config.json\" \"mistral-7b-sna/final_merged_checkpoint/tokenizer_config.json\"\n",
        "!cp \"mistral-7b-v0.1/special_tokens_map.json\" \"mistral-7b-sna/final_merged_checkpoint/special_tokens_map.json\""
      ],
      "metadata": {
        "id": "BO6pY_7aJAuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsHIuzqQpa3A"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do later: not working for now (but you're free to debug). You may need to delete the runtime to free the memory (since we will use a different implementation), especially if you are on the free colab."
      ],
      "metadata": {
        "id": "D8_djAnwhvv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95VmfELw5CXi",
        "outputId": "ae68a784-3056-4f37-acfa-a02dd8aed2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-_qt6yybs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-_qt6yybs\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 391177441b133645c02181b57370ab12f71b88c4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/vllm-project/vllm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As7Hf9s6Ik3m",
        "outputId": "28d5cdf4-50be-47cf-f5ee-5327be6ee5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/vllm-project/vllm\n",
            "  Cloning https://github.com/vllm-project/vllm to /tmp/pip-req-build-j4b1q0o4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/vllm-project/vllm /tmp/pip-req-build-j4b1q0o4\n",
            "  Resolved https://github.com/vllm-project/vllm to commit e2fb71ec9f2c3168ba8614408fa807a5f65707c5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja (from vllm==0.2.0)\n",
            "  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (5.9.5)\n",
            "Collecting ray>=2.5.1 (from vllm==0.2.0)\n",
            "  Downloading ray-2.7.0-cp310-cp310-manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (1.5.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (9.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (1.23.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers>=4.33.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (4.34.0.dev0)\n",
            "Collecting xformers>=0.0.22 (from vllm==0.2.0)\n",
            "  Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from vllm==0.2.0)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard] (from vllm==0.2.0)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.2.0) (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2->vllm==0.2.0) (4.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (4.19.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray>=2.5.1->vllm==0.2.0) (2.31.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->vllm==0.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->vllm==0.2.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->vllm==0.2.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->vllm==0.2.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->vllm==0.2.0) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->vllm==0.2.0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.1->vllm==0.2.0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.1->vllm==0.2.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.1->vllm==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.1->vllm==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.1->vllm==0.2.0) (4.66.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->vllm==0.2.0) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->vllm==0.2.0)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vllm==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vllm==0.2.0) (2023.3.post1)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]->vllm==0.2.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm==0.2.0) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm==0.2.0) (1.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.33.1->vllm==0.2.0) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->vllm==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->vllm==0.2.0) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.5.1->vllm==0.2.0) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.5.1->vllm==0.2.0) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.5.1->vllm==0.2.0) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.5.1->vllm==0.2.0) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=2.5.1->vllm==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=2.5.1->vllm==0.2.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=2.5.1->vllm==0.2.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->vllm==0.2.0) (1.3.0)\n",
            "Building wheels for collected packages: vllm\n",
            "  Building wheel for vllm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vllm: filename=vllm-0.2.0-cp310-cp310-linux_x86_64.whl size=17830322 sha256=c29194a82ef2532ea474b22015ebb56a2fdfe2d8562274e4c03610b0fff3a20f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1_fil1jb/wheels/5a/04/ea/4a4d084ce58ce6e350d358812d1b37f51729cafb43ee846c55\n",
            "Successfully built vllm\n",
            "Installing collected packages: ninja, websockets, uvloop, python-dotenv, httptools, h11, watchfiles, uvicorn, starlette, fastapi, ray, xformers, vllm\n",
            "Successfully installed fastapi-0.103.2 h11-0.14.0 httptools-0.6.0 ninja-1.11.1 python-dotenv-1.0.0 ray-2.7.0 starlette-0.27.0 uvicorn-0.23.2 uvloop-0.17.0 vllm-0.2.0 watchfiles-0.20.0 websockets-11.0.3 xformers-0.0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import os"
      ],
      "metadata": {
        "id": "t5iLkUP3InBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = \"mistral-7b-sna\""
      ],
      "metadata": {
        "id": "8-mtAxOQ7gk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_merged_dir = os.path.join(new_model_name, \"final_merged_checkpoint\")\n",
        "llm = LLM(output_merged_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqQSYyoFIoqJ",
        "outputId": "a4e38ece-91cf-4751-a1ca-22eafad1ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 09-29 15:21:34 llm_engine.py:72] Initializing an LLM engine with config: model='mistral-7b-sna/final_merged_checkpoint', tokenizer='mistral-7b-sna/final_merged_checkpoint', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 09-29 15:23:33 llm_engine.py:205] # GPU blocks: 9336, # CPU blocks: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=500)"
      ],
      "metadata": {
        "id": "62R-8MCaKPPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"\"\"Text: For a long time I used to go to bed early. Sometimes, when I had put out my candle, my eyes would close so quickly that I had not even time to say \"I'm going to sleep.\" And half an hour later the thought that it was time to go to sleep would awaken me; I would try to put away the book which, I imagined, was still in my hands, and to blow out the light; I had been thinking all the time, while I was asleep, of what I had just been reading, but my thoughts had run into a channel of their own, until I myself seemed actually to have become the subject of my book: a church, a quartet, the rivalry between François I and Charles V. This impression would persist for some moments after I was awake; it did not disturb my mind, but it lay like scales upon my eyes and prevented them from registering the fact that the candle was no longer burning. Then it would begin to seem unintelligible, as the thoughts of a former existence must be to a reincarnate spirit; the subject of my book would separate itself from me, leaving me free to choose whether I would form part of it or no; and at the same time my sight would return and I would be astonished to find myself in a state of darkness, pleasant and restful enough for the eyes, and even more, perhaps, for my mind, to which it appeared incomprehensible, without a cause, a matter dark indeed. \\n\\n### Analysis:\\n\\n\"\"\"]"
      ],
      "metadata": {
        "id": "G7udzfJyKTmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"\"\"Text: grandfather’s, who died years ago; and my body, the side\n",
        "upon which I was lying, faithful guardians of a past which\n",
        "my mind should never have forgotten, brought back\n",
        "before my eyes the glimmering flame of the night-light in\n",
        "its urn-shaped bowl of Bohemian glass that hung by\n",
        "chains from the ceiling, and the chimney-piece of Siena\n",
        "marble in my bedroom at Combray, in my grandparents’\n",
        "house, in those far distant days which at this moment I\n",
        "imagined to be in the present without being able to picture\n",
        "them exactly, and which would become plainer in a little\n",
        "while when I was properly awake.\n",
        "      Then the memory of a new position would spring up,\n",
        "and the wall would slide away in another direction; I was\n",
        "in my room in Mme de Saint-Loup’s house in the country;\n",
        "good heavens, it must be ten o’clock, they will have\n",
        "finished dinner! I must have overslept myself in the little\n",
        "nap which I always take when I come in from my walk\n",
        "with Mme de Saint-Loup, before dressing for the evening.\n",
        "For many years have now elapsed since the Combray days\n",
        "when, coming in from the longest and latest walks, I\n",
        "would still be in time to see the reflection of the sunset\n",
        "glowing in the panes of my bedroom window. It is a very\n",
        "different kind of life that one leads at Tansonville, at Mme\n",
        "de Saint-Loup’s, and a different kind of pleasure that I\n",
        "derive from taking walks only in the evenings, from\n",
        "visiting by moonlight the roads on which I used to play as\n",
        "a child in the sunshine; as for the bedroom in which I must\n",
        "have fallen asleep instead of dressing for dinner, I can see\n",
        "it from the distance as we return from our walk, with its\n",
        "lamp shining through the window, a solitary beacon in the\n",
        "night.\\n\\n### Analysis:\\n\\n\"\"\"]"
      ],
      "metadata": {
        "id": "k52-dNAOYmK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = llm.generate(prompts, sampling_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZovSigytKSI4",
        "outputId": "636f1919-b340-4946-ec44-b823c0efe43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "I718F_LS8Mkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f3f627-1955-49d0-cf00-843da1eb67b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[RequestOutput(request_id=0, prompt='Text: grandfather’s, who died years ago; and my body, the side\\nupon which I was lying, faithful guardians of a past which\\nmy mind should never have forgotten, brought back\\nbefore my eyes the glimmering flame of the night-light in\\nits urn-shaped bowl of Bohemian glass that hung by\\nchains from the ceiling, and the chimney-piece of Siena\\nmarble in my bedroom at Combray, in my grandparents’\\nhouse, in those far distant days which at this moment I\\nimagined to be in the present without being able to picture\\nthem exactly, and which would become plainer in a little\\nwhile when I was properly awake.\\n      Then the memory of a new position would spring up,\\nand the wall would slide away in another direction; I was\\nin my room in Mme de Saint-Loup’s house in the country;\\ngood heavens, it must be ten o’clock, they will have\\nfinished dinner! I must have overslept myself in the little\\nnap which I always take when I come in from my walk\\nwith Mme de Saint-Loup, before dressing for the evening.\\nFor many years have now elapsed since the Combray days\\nwhen, coming in from the longest and latest walks, I\\nwould still be in time to see the reflection of the sunset\\nglowing in the panes of my bedroom window. It is a very\\ndifferent kind of life that one leads at Tansonville, at Mme\\nde Saint-Loup’s, and a different kind of pleasure that I\\nderive from taking walks only in the evenings, from\\nvisiting by moonlight the roads on which I used to play as\\na child in the sunshine; as for the bedroom in which I must\\nhave fallen asleep instead of dressing for dinner, I can see\\nit from the distance as we return from our walk, with its\\nlamp shining through the window, a solitary beacon in the\\nnight.\\n\\n### Analysis:\\n\\n', prompt_token_ids=[1, 7379, 28747, 19300, 28809, 28713, 28725, 693, 4847, 1267, 3584, 28745, 304, 586, 2187, 28725, 272, 2081, 13, 715, 266, 690, 315, 403, 10580, 28725, 26073, 6980, 3693, 302, 264, 2609, 690, 13, 1916, 2273, 1023, 1484, 506, 13652, 28725, 4248, 852, 13, 12098, 586, 2282, 272, 1272, 14917, 288, 23570, 302, 272, 2125, 28733, 3646, 297, 13, 1046, 28705, 453, 28733, 21501, 13494, 302, 3268, 6981, 753, 5252, 369, 7342, 486, 13, 338, 1606, 477, 272, 15273, 28725, 304, 272, 21367, 2705, 28733, 17152, 302, 318, 1375, 28708, 13, 3479, 982, 297, 586, 9384, 438, 19422, 919, 28725, 297, 586, 3487, 20415, 28809, 13, 6186, 28725, 297, 1395, 2082, 15569, 2202, 690, 438, 456, 2470, 315, 13, 25069, 1311, 298, 347, 297, 272, 2169, 1671, 1250, 2358, 298, 5754, 13, 1237, 28719, 4668, 28725, 304, 690, 682, 2727, 549, 2542, 297, 264, 1628, 13, 5413, 739, 315, 403, 9222, 20875, 28723, 13, 355, 2479, 272, 4733, 302, 264, 633, 2840, 682, 7474, 582, 28725, 13, 391, 272, 3500, 682, 16513, 1753, 297, 1698, 5007, 28745, 315, 403, 13, 262, 586, 2003, 297, 351, 1127, 340, 6393, 28733, 28758, 14245, 28809, 28713, 2134, 297, 272, 2939, 28745, 13, 12684, 9425, 596, 28725, 378, 1580, 347, 3095, 289, 28809, 10487, 28725, 590, 622, 506, 13, 6159, 7854, 28808, 315, 1580, 506, 13775, 291, 447, 3561, 297, 272, 1628, 13, 28711, 377, 690, 315, 1743, 1388, 739, 315, 1567, 297, 477, 586, 2338, 13, 3415, 351, 1127, 340, 6393, 28733, 28758, 14245, 28725, 1159, 21993, 354, 272, 6856, 28723, 13, 2565, 1287, 1267, 506, 1055, 639, 11834, 1854, 272, 19422, 919, 2202, 13, 11233, 28725, 3524, 297, 477, 272, 23397, 304, 7345, 16830, 28725, 315, 13, 28727, 474, 1309, 347, 297, 727, 298, 1032, 272, 17271, 302, 272, 4376, 673, 13, 1727, 15675, 297, 272, 3961, 274, 302, 586, 9384, 2924, 28723, 661, 349, 264, 1215, 13, 28715, 15791, 2112, 302, 1411, 369, 624, 8681, 438, 320, 21140, 5485, 28725, 438, 351, 1127, 13, 450, 6393, 28733, 28758, 14245, 28809, 28713, 28725, 304, 264, 1581, 2112, 302, 10736, 369, 315, 13, 21303, 477, 3344, 16830, 865, 297, 272, 1019, 742, 28725, 477, 13, 3225, 4328, 486, 11375, 3646, 272, 15014, 356, 690, 315, 1307, 298, 1156, 390, 13, 28708, 1502, 297, 272, 4376, 22121, 28745, 390, 354, 272, 9384, 297, 690, 315, 1580, 13, 15378, 12962, 15231, 3519, 302, 21993, 354, 7854, 28725, 315, 541, 1032, 13, 279, 477, 272, 5328, 390, 478, 604, 477, 813, 2338, 28725, 395, 871, 13, 28714, 1057, 26575, 1059, 272, 2924, 28725, 264, 2128, 11969, 347, 10364, 297, 272, 13, 6988, 28723, 13, 13, 27332, 19442, 28747, 13, 13], outputs=[CompletionOutput(index=0, text=\"Summary: The narrator is remembering their past and the different rooms they have slept in.\\nNarrative arc: Reflective\\nEnunciation: First-person narrative\\nTone: Reflective\\nGenre: Memoir\\nIntertextuality: Memoir\\nSpeech standard: Standard\\nLiterary form: Stream of consciousness\\nLiterary movement: Realism\\nActive character: The narrator\\nTime setting: The past\\nFuzzy time: Nonspecific moment\\nFuzzy place: Combray, Mme de Saint-Loup's house in the country, Tansonville\\nFuzzy die: Nonspecific\\nFuzzy action: Nonspecific\\nFuzzy level of consciousness: Nonspecific\\nFuzzy fuzzy time: Nonspecific moment\\nFuzzy fuzzy place: Nonspecific place\\nFuzzy fuzzy die: Nonspecific\\nFuzzy fuzzy action: Nonspecific action\\nFuzzy fuzzy level of consciousness: Nonspecific level of consciousness\\nFuzzy fuzzy time: Nonspecific time\\nFuzzy fuzzy place: Nonspecific place\\nFuzzy fuzzy die: Nonspecific die\\nFuzzy fuzzy action: Nonspecific action\\nFuzzy fuzzy level of consciousness: Nonspecific level of consciousness\\nFuzzy fuzzy time: Nonspecific time\\nFuzzy fuzzy place: Nonspecific place\\nFuzzy fuzzy die: Nonspecific die\\nFuzzy fuzzy action: Nonspecific action\\nFuzzy fuzzy level of consciousness: Nonspecific level of consciousness\\nFuzzy fuzzy time: Nonspecific time\\nFuzzy fuzzy place: Nonspecific place\\nFuzzy fuzzy die: Nonspecific die\\nFuzzy fuzzy action: Nonspecific action\\nFuzzy fuzzy level of consciousness: Nonspecific level of consciousness\\nFuzzy fuzzy time: Nonspecific time\\nFuzzy fuzzy place: Nonspecific place\\nFuzzy fuzzy die: Nonspecific die\\n\", token_ids=[17590, 28747, 415, 9819, 1028, 349, 26036, 652, 2609, 304, 272, 1581, 9698, 590, 506, 18283, 297, 28723, 13, 28759, 2654, 1197, 13605, 28747, 6360, 844, 495, 13, 1608, 14281, 352, 28747, 4205, 28733, 9701, 15529, 13, 28738, 538, 28747, 6360, 844, 495, 13, 9872, 267, 28747, 6011, 7236, 13, 3355, 772, 840, 472, 28747, 6011, 7236, 13, 24812, 5295, 4787, 28747, 12623, 13, 28758, 1685, 628, 1221, 28747, 15915, 302, 16508, 13, 28758, 1685, 628, 6249, 28747, 7960, 1443, 13, 8051, 3233, 28747, 415, 9819, 1028, 13, 2098, 5587, 28747, 415, 2609, 13, 28765, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 2470, 13, 28765, 3533, 2140, 1633, 28747, 19422, 919, 28725, 351, 1127, 340, 6393, 28733, 28758, 14245, 28742, 28713, 2134, 297, 272, 2939, 28725, 320, 21140, 5485, 13, 28765, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 13, 28765, 3533, 2140, 2992, 28747, 418, 1053, 841, 921, 13, 28765, 3533, 2140, 2184, 302, 16508, 28747, 418, 1053, 841, 921, 13, 28765, 3533, 2140, 285, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 2470, 13, 28765, 3533, 2140, 285, 3533, 2140, 1633, 28747, 418, 1053, 841, 921, 1633, 13, 28765, 3533, 2140, 285, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 13, 28765, 3533, 2140, 285, 3533, 2140, 2992, 28747, 418, 1053, 841, 921, 2992, 13, 28765, 3533, 2140, 285, 3533, 2140, 2184, 302, 16508, 28747, 418, 1053, 841, 921, 2184, 302, 16508, 13, 28765, 3533, 2140, 285, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 727, 13, 28765, 3533, 2140, 285, 3533, 2140, 1633, 28747, 418, 1053, 841, 921, 1633, 13, 28765, 3533, 2140, 285, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 1202, 13, 28765, 3533, 2140, 285, 3533, 2140, 2992, 28747, 418, 1053, 841, 921, 2992, 13, 28765, 3533, 2140, 285, 3533, 2140, 2184, 302, 16508, 28747, 418, 1053, 841, 921, 2184, 302, 16508, 13, 28765, 3533, 2140, 285, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 727, 13, 28765, 3533, 2140, 285, 3533, 2140, 1633, 28747, 418, 1053, 841, 921, 1633, 13, 28765, 3533, 2140, 285, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 1202, 13, 28765, 3533, 2140, 285, 3533, 2140, 2992, 28747, 418, 1053, 841, 921, 2992, 13, 28765, 3533, 2140, 285, 3533, 2140, 2184, 302, 16508, 28747, 418, 1053, 841, 921, 2184, 302, 16508, 13, 28765, 3533, 2140, 285, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 727, 13, 28765, 3533, 2140, 285, 3533, 2140, 1633, 28747, 418, 1053, 841, 921, 1633, 13, 28765, 3533, 2140, 285, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 1202, 13, 28765, 3533, 2140, 285, 3533, 2140, 2992, 28747, 418, 1053, 841, 921, 2992, 13, 28765, 3533, 2140, 285, 3533, 2140, 2184, 302, 16508, 28747, 418, 1053, 841, 921, 2184, 302, 16508, 13, 28765, 3533, 2140, 285, 3533, 2140, 727, 28747, 418, 1053, 841, 921, 727, 13, 28765, 3533, 2140, 285, 3533, 2140, 1633, 28747, 418, 1053, 841, 921, 1633, 13, 28765, 3533, 2140, 285, 3533, 2140, 1202, 28747, 418, 1053, 841, 921, 1202, 13], cumulative_logprob=-3.0976137667894363, logprobs={}, finish_reason=length)], finished=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application du modèle à un jeu de données"
      ],
      "metadata": {
        "id": "POU0xwoF11vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "proust = pd.read_excel(\"proust_novel.xlsx\")"
      ],
      "metadata": {
        "id": "eXQLAu8R14Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "\n",
        "for texts in proust[\"text\"].tolist():\n",
        "  prompts.append(\"Text: \" + texts + \"\\n\\n### Analysis:\\n\\n\")\n",
        "\n",
        "print(prompts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC6uthvn17lq",
        "outputId": "0da117e7-6b9e-4044-b8f0-261cf710d62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:               MARCEL PROUST\n",
            "\n",
            "     Marcel Proust was born in the Parisian suburb of\n",
            "Auteuil on July 10, 1871. His father, Adrien Proust, was a\n",
            "doctor celebrated for his work in epidemiology; his\n",
            "mother, Jeanne Weil, was a stockbroker’s daughter of\n",
            "Jewish descent. He lived as a child in the family home on\n",
            "Boulevard Malesherbes in Paris, but spent vacations with\n",
            "his aunt and uncle in the town of Illiers near Chartres,\n",
            "where the Prousts had lived for generations and which\n",
            "became the model for the Combray of his great novel. (In\n",
            "recent years it was officially renamed Illiers-Combray.)\n",
            "Sickly from birth, Marcel was subject from the age of nine\n",
            "to violent attacks of asthma, and although he did a year of\n",
            "military service as a young man and studied law and\n",
            "political science, his invalidism disqualified him from an\n",
            "active professional life.\n",
            "     During the 1890s Proust contributed sketches to Le\n",
            "Figaro and to a short-lived magazine, Le Banquet,\n",
            "founded by some of his school friends in 1892. Pleasures\n",
            "and Days, a collection of his stories, essays, and poems,\n",
            "was published in 1896. In his youth Proust led an active\n",
            "social life, penetrating the highest circles of wealth and\n",
            "aristocracy. Artistically and intellectually, his influences\n",
            "included the aesthetic criticism of John Ruskin, the\n",
            "philosophy of Henri Bergson, the music of Wagner, and\n",
            "                            vii\n",
            "\n",
            "\n",
            "### Analysis:\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = llm.generate(prompts, sampling_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZfsLfuj2QeR",
        "outputId": "f97900ea-3dd7-4d30-9e2d-a38c18ad360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 589/589 [02:25<00:00,  4.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proust[\"analysis\"] = outputs"
      ],
      "metadata": {
        "id": "XKXOmSz24TVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proust"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68756
        },
        "id": "8RFLS0lckTge",
        "outputId": "9bdb0084-4d80-4e95-f4df-7a30619e5142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0                                               text  page  \\\n",
              "0             0                MARCEL PROUST\\n\\n     Marcel Pro...     9   \n",
              "1             1  the fiction of Anatole France (on whom he mode...    10   \n",
              "2             2  Goncourt Prize, bringing Proust great and inst...    11   \n",
              "3             3                        CONTENTS\\n\\nNote on the ...    15   \n",
              "4             4                 Note on the Translation (1981)\\...    17   \n",
              "..          ...                                                ...   ...   \n",
              "584         584  576                     SWANN’S WAY\\n\\npiled a...   600   \n",
              "585         585                   PLACE-NAMES · THE NAME       ...   601   \n",
              "586         586  578                      SWANN’S WAY\\n\\ngrey s...   602   \n",
              "587         587                   PLACE-NAMES · THE NAME       ...   603   \n",
              "588         588  580                     SWANN’S WAY\\n\\nof a pa...   604   \n",
              "\n",
              "           title         author  \\\n",
              "0    Swann's Way  Marcel Proust   \n",
              "1    Swann's Way  Marcel Proust   \n",
              "2    Swann's Way  Marcel Proust   \n",
              "3    Swann's Way  Marcel Proust   \n",
              "4    Swann's Way  Marcel Proust   \n",
              "..           ...            ...   \n",
              "584  Swann's Way  Marcel Proust   \n",
              "585  Swann's Way  Marcel Proust   \n",
              "586  Swann's Way  Marcel Proust   \n",
              "587  Swann's Way  Marcel Proust   \n",
              "588  Swann's Way  Marcel Proust   \n",
              "\n",
              "                                              analysis  \n",
              "0    RequestOutput(request_id=1, prompt='Text:     ...  \n",
              "1    RequestOutput(request_id=2, prompt='Text: the ...  \n",
              "2    RequestOutput(request_id=3, prompt='Text: Gonc...  \n",
              "3    RequestOutput(request_id=4, prompt='Text:     ...  \n",
              "4    RequestOutput(request_id=5, prompt='Text:     ...  \n",
              "..                                                 ...  \n",
              "584  RequestOutput(request_id=585, prompt='Text: 57...  \n",
              "585  RequestOutput(request_id=586, prompt='Text:   ...  \n",
              "586  RequestOutput(request_id=587, prompt='Text: 57...  \n",
              "587  RequestOutput(request_id=588, prompt='Text:   ...  \n",
              "588  RequestOutput(request_id=589, prompt='Text: 58...  \n",
              "\n",
              "[589 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cea96bd4-414b-4fa4-9a5d-07649cb689b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>page</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>analysis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MARCEL PROUST\\n\\n     Marcel Pro...</td>\n",
              "      <td>9</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=1, prompt='Text:     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the fiction of Anatole France (on whom he mode...</td>\n",
              "      <td>10</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=2, prompt='Text: the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Goncourt Prize, bringing Proust great and inst...</td>\n",
              "      <td>11</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=3, prompt='Text: Gonc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>CONTENTS\\n\\nNote on the ...</td>\n",
              "      <td>15</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=4, prompt='Text:     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Note on the Translation (1981)\\...</td>\n",
              "      <td>17</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=5, prompt='Text:     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>584</td>\n",
              "      <td>576                     SWANN’S WAY\\n\\npiled a...</td>\n",
              "      <td>600</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=585, prompt='Text: 57...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>585</th>\n",
              "      <td>585</td>\n",
              "      <td>PLACE-NAMES · THE NAME       ...</td>\n",
              "      <td>601</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=586, prompt='Text:   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>586</th>\n",
              "      <td>586</td>\n",
              "      <td>578                      SWANN’S WAY\\n\\ngrey s...</td>\n",
              "      <td>602</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=587, prompt='Text: 57...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>587</td>\n",
              "      <td>PLACE-NAMES · THE NAME       ...</td>\n",
              "      <td>603</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=588, prompt='Text:   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588</th>\n",
              "      <td>588</td>\n",
              "      <td>580                     SWANN’S WAY\\n\\nof a pa...</td>\n",
              "      <td>604</td>\n",
              "      <td>Swann's Way</td>\n",
              "      <td>Marcel Proust</td>\n",
              "      <td>RequestOutput(request_id=589, prompt='Text: 58...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>589 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cea96bd4-414b-4fa4-9a5d-07649cb689b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cea96bd4-414b-4fa4-9a5d-07649cb689b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cea96bd4-414b-4fa4-9a5d-07649cb689b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9523aaa2-9061-4c10-8466-cdd77990fa7a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9523aaa2-9061-4c10-8466-cdd77990fa7a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9523aaa2-9061-4c10-8466-cdd77990fa7a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proust.to_excel(\"proust_novel_mistral_3.xlsx\")"
      ],
      "metadata": {
        "id": "WOD1qcm24VK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db6175917fdf481693d30d258cd1c7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d543faeb3a0647d8833c4e4417c6f88b",
              "IPY_MODEL_9353efae551e413c8f95259906b9deeb",
              "IPY_MODEL_fed382f6cb0a4b839ace317b8b96e5b3"
            ],
            "layout": "IPY_MODEL_3639468703a043b5bb819076f45f375d"
          }
        },
        "d543faeb3a0647d8833c4e4417c6f88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392b15d35cfc45398e64cf6e315a4bc6",
            "placeholder": "​",
            "style": "IPY_MODEL_1a633383d08a4eb28e7bad389c2fda55",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9353efae551e413c8f95259906b9deeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50119634a30e4a3db374662df8f61e03",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db5aa8df2ab14f49b7bc08191ff11227",
            "value": 2
          }
        },
        "fed382f6cb0a4b839ace317b8b96e5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7cbef2170ef466bb646906f97b6a0ee",
            "placeholder": "​",
            "style": "IPY_MODEL_e61c36bd89e74d819b6a81f8aeeef544",
            "value": " 2/2 [00:19&lt;00:00,  9.01s/it]"
          }
        },
        "3639468703a043b5bb819076f45f375d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392b15d35cfc45398e64cf6e315a4bc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a633383d08a4eb28e7bad389c2fda55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50119634a30e4a3db374662df8f61e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5aa8df2ab14f49b7bc08191ff11227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7cbef2170ef466bb646906f97b6a0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61c36bd89e74d819b6a81f8aeeef544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bacea8f68dd4d2cab09775540f7786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bb052c616c84851a0991c2730753d26",
              "IPY_MODEL_fbc861bb471f481a8b7cbb9d90a2b9bd",
              "IPY_MODEL_22ee3080750f4070bea559a22107046a"
            ],
            "layout": "IPY_MODEL_69f4669e71b64cb08633b33fe6815473"
          }
        },
        "1bb052c616c84851a0991c2730753d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5638ff078eb43c2a126e8de3bf97b4e",
            "placeholder": "​",
            "style": "IPY_MODEL_f4f6e6d0f5e84ac3b8a5f0c0175c012f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fbc861bb471f481a8b7cbb9d90a2b9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef99f3030f941f48e90a7cb07511374",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efd96969d87148fa8109b573ec16217f",
            "value": 2
          }
        },
        "22ee3080750f4070bea559a22107046a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8c483e05a643b09b030b6c3375a107",
            "placeholder": "​",
            "style": "IPY_MODEL_fae0a3e73eeb410195d04e6e7f85d44f",
            "value": " 2/2 [00:23&lt;00:00, 10.84s/it]"
          }
        },
        "69f4669e71b64cb08633b33fe6815473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5638ff078eb43c2a126e8de3bf97b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4f6e6d0f5e84ac3b8a5f0c0175c012f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ef99f3030f941f48e90a7cb07511374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd96969d87148fa8109b573ec16217f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b8c483e05a643b09b030b6c3375a107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae0a3e73eeb410195d04e6e7f85d44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}